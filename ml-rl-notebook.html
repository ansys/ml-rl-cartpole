

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Reinforcement Machine Learning using PyMAPDL: Cartpole Simulation &#8212; ML-RL-Cartpole</title>
    
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/ansys_sphinx_theme.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.datatables.net/1.10.23/css/jquery.dataTables.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    
        <link href="_static/css/breadcrumbs.css" rel="stylesheet">
    
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/js/table.js"></script>
    <script src="https://cdn.datatables.net/1.10.23/js/jquery.dataTables.min.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'ml-rl-notebook';</script>
    <link rel="shortcut icon" href="_static/ansys-favicon.png"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Reinforcement ML using PyMAPDL: Cart-Pole Simulation" href="index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container"><!-- Debugging: html_theme_options =  -->

  <!-- If there is no MiliSearch enabled, use the PyData search -->
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
  </div>
  
    
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  <div class="navbar-header-items__start">
    
      <div class="navbar-item">
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/pyansys-logo-black-cropped.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/pyansys-logo-black-cropped.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
    
  </div>
  
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="#">
                        Reinforcement Machine Learning using PyMAPDL: Cartpole Simulation
                      </a>
                    </li>
                
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
        </div>
      
      
        <div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pyansys/ml-rl-cartpole" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-brands fa-square-github"></i></span>
            <label class="sr-only">GitHub</label></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
    </div>
  

  
    <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fa-solid fa-outdent"></span>
    </label>
  
</div>

    </nav>
  

    

    
    <div class="container-xl">
    <div class="row">
        <div class="col-12 col-md-3" id="breadcrumbs-spacer"></div>
        <div class="col-12 col-md-11 col-xl-9" id="breadcrumbs">
          
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
    
        <li class="nav-item nav-item-additional-breadcrumb"><a href="https://docs.pyansys.com/">PyAnsys</a> » </li>
    
        <li class="nav-item nav-item-additional-breadcrumb"><a href="https://mapdl.docs.pyansys.com/version/stable/">PyMAPDL</a> » </li>
    
        <li class="nav-item nav-item-additional-breadcrumb"><a href="https://mapdl.docs.pyansys.com/version/stable/examples/index.html">Examples Library</a> » </li>
    
    
        <li class="nav-item nav-item-0"><a href="index.html">ML-RL-Cartpole</a> » </li>
    

        <li class="nav-item nav-item-this"><a href="">Reinforcement Machine Learning using PyMAPDL: Cartpole Simulation</a></li> 
      </ul>
    </div>
        </div>
  </div>
</div>
    

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item current active">
                      <a class="nav-link nav-internal" href="#">
                        Reinforcement Machine Learning using PyMAPDL: Cartpole Simulation
                      </a>
                    </li>
                
  </ul>
</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links navbar-nav"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pyansys/ml-rl-cartpole" title="GitHub" class="nav-link" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><span><i class="fa-brands fa-square-github"></i></span>
            <label class="sr-only">GitHub</label></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article"></div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="reinforcement-machine-learning-using-pymapdl-cartpole-simulation">
<span id="ref-ml-rl-cartpole"></span><h1>Reinforcement Machine Learning using PyMAPDL: Cartpole Simulation<a class="headerlink" href="#reinforcement-machine-learning-using-pymapdl-cartpole-simulation" title="Permalink to this headline">#</a></h1>
<p>This notebook demonstrates a reinforcement machine learning example using MAPDL
through PyMAPDL.</p>
<p>This example assumes you have cloned the <a class="reference external" href="https://github.com/pyansys/ml-rl-cartpole">ml-rl-cartpole</a> repository and are in the correct
directory with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">git</span> <span class="n">clone</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">pyansys</span><span class="o">/</span><span class="n">ml</span><span class="o">-</span><span class="n">rl</span><span class="o">-</span><span class="n">cartpole</span>
<span class="n">cd</span> <span class="n">ml</span><span class="o">-</span><span class="n">rl</span><span class="o">-</span><span class="n">cartpole</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Installing pyansys packages</span>
<span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>./pyansys_rl<span class="w"> </span>./pyansys_gym<span class="w"> </span>-q<span class="w"> </span>--user<span class="w"> </span>--use-feature<span class="o">=</span><span class="k">in</span>-tree-build
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Loading requires packages</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>

<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">pyansys_cartpole</span>

<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
<section id="background-markov-decision-process">
<h2>Background: Markov Decision Process<a class="headerlink" href="#background-markov-decision-process" title="Permalink to this headline">#</a></h2>
<figure class="align-default" id="id1">
<a class="reference internal image-reference" href="_images/MDP_board.jpg"><img alt="Fig 1" src="_images/MDP_board.jpg" style="width: 217.5px; height: 212.0px;" /></a>
<figcaption>
<p><span class="caption-text">Fig 1: Learning agent.</span><a class="headerlink" href="#id1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>In a Markov Decision Process we have an agent immersed in an
environment. At any given time, the agent finds itself in a state and it
must select one of the available actions. Upon taking an action, the
environment responds by assigning a reward and transitioning the agent to
a successor state. This loop continues until a terminal state is
reached. It is interesting to ask: could we learn to act optimally in
such a setup? could we learn to select sequences of actions that
maximize long term cumulative rewards?</p>
<figure class="align-default" id="id2">
<a class="reference internal image-reference" href="_images/MDP_loop.jpg"><img alt="Fig 2" src="_images/MDP_loop.jpg" style="width: 470.0px; height: 329.5px;" /></a>
<figcaption>
<p><span class="caption-text">Fig 2: Markov Decision Process in Reinforced Learning</span><a class="headerlink" href="#id2" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="cartpole-simulation">
<h2>CartPole Simulation<a class="headerlink" href="#cartpole-simulation" title="Permalink to this headline">#</a></h2>
<p>The CartPole is a classic control problem. It is a balancing task: push
the cart such that the pinned pole remains upright. In other words, the
pole behaves as a solid inverted pendulum and is unstable about the
desired configuration. A simple implementation could use a
revolute/hinge joint between the cart and the pole, and a translational
joint between the cart and the ground.</p>
<figure class="align-default" id="id3">
<a class="reference internal image-reference" href="_images/cartpole_description.jpg"><img alt="Fig 3" src="_images/cartpole_description.jpg" style="width: 588.0px; height: 294.0px;" /></a>
<figcaption>
<p><span class="caption-text">Fig 3: Cartpole problem.</span><a class="headerlink" href="#id3" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<section id="mapdl-in-the-loop">
<h3>MAPDL in the loop<a class="headerlink" href="#mapdl-in-the-loop" title="Permalink to this headline">#</a></h3>
<p>In this implementation of the CartPole as a Markov Decision Process, we
highlight the following components:</p>
<ul class="simple">
<li><p>Actions: push either left (<span class="math notranslate nohighlight">\(action = 0\)</span>) or right
(<span class="math notranslate nohighlight">\(action = 1\)</span>)</p></li>
<li><p>State:
<span class="math notranslate nohighlight">\(x_{\text{cart}}, v_{\text{cart}}, \theta_{\text{pole}}, v_{\text{pole}}\)</span></p></li>
<li><p>Reward: +1 for every timestep still in equilibrium</p></li>
<li><p>Transition Model: courtesy of an MAPDL structural transient analysis</p></li>
</ul>
<p>At each episode, the system starts in a randomly seeded state, with
positions, velocities and angles picked from a uniform distribution
about the vertical/resting position, thus it is unlikely to ever be at
equilibrium. Even if it were, the equilibrium would be unstable.</p>
<figure class="align-default" id="id4">
<a class="reference internal image-reference" href="_images/ANSYS_loop.jpg"><img alt="Fig 4" src="_images/ANSYS_loop.jpg" style="width: 478.0px; height: 319.0px;" /></a>
<figcaption>
<p><span class="caption-text">Fig 4: A single iteration of the CartPole as a Markov Decision Process using MAPDL</span><a class="headerlink" href="#id4" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="instance-creation-mapdl-in-the-loop">
<h3>Instance creation: MAPDL in the loop<a class="headerlink" href="#instance-creation-mapdl-in-the-loop" title="Permalink to this headline">#</a></h3>
<p>Create an instance of an MAPDL environment that is specially wrapped for
use in <a class="reference external" href="https://gym.openai.com/">OpenAI Gym</a> thanks to the newly
developed python gRPC bindings
(<a class="reference external" href="https://github.com/pyansys">PyAnsys</a>). The wrapper sets up the
CartPole physics, accepts the available actions (i.e. forces), and
calculates the state transitions (kinematic response) after every time
step (an MAPDL load step).</p>
<p>For reference, OpenAI Gym provides its own ad hoc
<a class="reference external" href="https://gym.openai.com/envs/CartPole-v1/">environment</a> for solving
the system’s <a class="reference external" href="https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py">kinematic
equations</a></p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Loading environment</span>
<span class="n">env_name</span> <span class="o">=</span> <span class="s1">&#39;pyansys-CartPole-v0&#39;</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">env_name</span><span class="p">)</span>

<span class="c1"># run several episodes (e.g., 3) of the CartPole using a random action, i.e., sometimes 0 (push left), sometimes 1 (push right)</span>
<span class="n">n_episodes</span> <span class="o">=</span> <span class="mi">3</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_episodes</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;*&#39;</span> <span class="o">*</span> <span class="mi">30</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;Episode: </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;*&#39;</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>
    <span class="n">cur_state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">done</span><span class="p">,</span> <span class="n">r_tot</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;State:&#39;</span><span class="p">,</span> <span class="n">cur_state</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\t</span><span class="s1">Action:&#39;</span><span class="p">,</span> <span class="s1">&#39;---&gt;&#39;</span> <span class="k">if</span> <span class="n">action</span> <span class="k">else</span> <span class="s1">&#39;&lt;---&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\t</span><span class="s1">Reward: &#39;</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
        <span class="n">cur_state</span><span class="p">,</span> <span class="n">r_tot</span> <span class="o">=</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">r_tot</span> <span class="o">+</span> <span class="n">reward</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Episode Reward:&#39;</span><span class="p">,</span> <span class="n">r_tot</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
</pre></div>
</div>
<pre class="literal-block"><strong>**************************</strong> Episode: 1 <strong>**************************</strong>
State: [-0.0168  0.045   0.9473  0.    ]    Action: &lt;---    Reward:  1
State: [-0.0179 -0.1075  1.5645  0.0392]    Action: ---&gt;    Reward:  1
State: [-0.0199 -0.0881  2.6821  0.042 ]    Action: &lt;---    Reward:  1
State: [-0.0218 -0.1082  3.8081  0.047 ]    Action: &lt;---    Reward:  1
State: [-0.0259 -0.3035  6.1548  0.132 ]    Action: ---&gt;    Reward:  1
State: [-0.0318 -0.2846  9.4723  0.1304]    Action: ---&gt;    Reward:  1
State: [-0.0356 -0.0901 11.5597  0.0742]    Action: &lt;---    Reward:  1
Episode Reward: 7

<strong>**************************</strong> Episode: 2 <strong>**************************</strong>
State: [ 0.0162 -0.0124 -1.0163  0.    ]    Action: ---&gt;    Reward:  1
State: [ 0.0173  0.1075 -1.6337  0.0394]    Action: &lt;---    Reward:  1
State: [ 0.0192  0.0882 -2.7514  0.0425]    Action: ---&gt;    Reward:  1
State: [ 0.0212  0.1083 -3.8778  0.0478]    Action: &lt;---    Reward:  1
State: [ 0.0231  0.0888 -5.0001  0.0595]    Action: ---&gt;    Reward:  1
State: [ 0.0251  0.1092 -6.1301  0.0614]    Action: &lt;---    Reward:  1
State: [ 0.0271  0.0897 -7.2555  0.0785]    Action: &lt;---    Reward:  1
State: [ 0.027  -0.1045 -7.1719  0.0084]    Action: &lt;---    Reward:  1
State: [ 0.0229 -0.2994 -4.876   0.0446]    Action: ---&gt;    Reward:  1
State: [ 0.0172 -0.2792 -1.5695  0.0351]    Action: ---&gt;    Reward:  1
State: [ 0.0135 -0.0839  0.5111  0.0432]    Action: &lt;---    Reward:  1
State: [ 0.0116 -0.1032  1.5847  0.0448]    Action: &lt;---    Reward:  1
State: [ 0.0076 -0.2979  3.8727  0.0274]    Action: &lt;---    Reward:  1
State: [-0.0003 -0.4931  8.3327  0.0826]    Action: ---&gt;    Reward:  1
Episode Reward: 14

<strong>**************************</strong> Episode: 3 <strong>**************************</strong>
State: [ 0.0363 -0.0121 -0.0026  0.    ]    Action: &lt;---    Reward:  1
State: [ 0.0352 -0.1074  0.6139  0.0367]    Action: ---&gt;    Reward:  1
State: [ 0.0333 -0.0878  1.7286  0.0349]    Action: ---&gt;    Reward:  1
State: [0.0335 0.1071 1.6179 0.038 ]        Action: ---&gt;    Reward:  1
State: [ 0.0376  0.3024 -0.7279  0.1012]    Action: &lt;---    Reward:  1
State: [ 0.0434  0.2828 -4.0709  0.1019]    Action: ---&gt;    Reward:  1
State: [ 0.0493  0.3028 -7.3943  0.1046]    Action: ---&gt;    Reward:  1
State: [  0.0573   0.498  -11.8517   0.1887]        Action: &lt;---    Reward:  1
Episode Reward: 8</pre>
<p>The above results express the actions taken by the agent and the
correpondent rewards and states.</p>
</section>
</section>
<section id="reinforcement-learning-deep-q-network">
<h2>Reinforcement Learning: Deep Q-Network<a class="headerlink" href="#reinforcement-learning-deep-q-network" title="Permalink to this headline">#</a></h2>
<p>The Deep Q-Network (DQN) implementation is inspired by the famous paper
<a class="reference external" href="https://www.nature.com/articles/nature14236">Mnih et al, 2014</a>.</p>
<div class="line-block">
<div class="line">The algorithm consists of two neural networks (NN) to accumulate the
statistical information about optimal reward sequences. One NN, used
for learning, is trained at every step by picking a random sample of
experiences (state, action, reward)-tuples from an “experience
buffer”.</div>
<div class="line">The buffer is filled from new experiences (old experiences are retired
once the contents exceed capacity) as the agent trains.</div>
<div class="line">At the outset, the buffer is pre-filled with experiences from an agent
that behaves at random for a fixed number of steps.</div>
</div>
<figure class="align-default" id="id5">
<a class="reference internal image-reference" href="_images/DQN_clean.jpg"><img alt="Fig 5" src="_images/DQN_clean.jpg" style="width: 418.5px; height: 292.0px;" /></a>
<figcaption>
<p><span class="caption-text">Fig 5: Learning buffer.</span><a class="headerlink" href="#id5" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The other NN, used for behavior, is just a snapshot of the first
refreshed at fixed intervals and dictates what actions to take.</p>
<section id="neural-network-parameters">
<h3>Neural Network Parameters<a class="headerlink" href="#neural-network-parameters" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Neural network size/topology: [4, 32, 32, 2]</p>
<ul>
<li><p>4: for each components of the state
(<span class="math notranslate nohighlight">\(x_{\text{cart}}, v_{\text{cart}}, \theta_{\text{pole}}, v_{\text{pole}}\)</span>)</p></li>
<li><p>32: hidden layer 1 with 32 ReLU neurons</p></li>
<li><p>32: hidden layer 2 with 32 ReLU neurons</p></li>
<li><p>2: for each possible action with linear neurons [Left, Right]</p></li>
</ul>
</li>
<li><p>Neural network refresh rate: every timestep</p></li>
<li><p>Buffer capacity: 40K experiences</p></li>
<li><p>Buffer warmup: 1K experiences</p></li>
<li><p>Sample size: 64 experiences per timestep</p></li>
<li><p><span class="math notranslate nohighlight">\(\varepsilon\)</span>: linear decay from 1 to .05 in 1000 timestep (.05
thereafter)</p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma = .99\)</span></p></li>
</ul>
</section>
<section id="learning-task">
<h3>Learning Task<a class="headerlink" href="#learning-task" title="Permalink to this headline">#</a></h3>
<p>Try to balance the pole up to 200 steps. Declare success when the
10-episode average exceeds 196</p>
<p>Each learner will display a history of performance per episode, encoded
as follows:</p>
<ul class="simple">
<li><p>e: episode number</p></li>
<li><p>n: timesteps</p></li>
<li><p>x: max reward thus far</p></li>
<li><p>r: reward</p></li>
<li><p>v: avg reward (e.g., for last 10 steps)</p></li>
<li><p>vx: max average reward</p></li>
<li><p>nt: total timesteps thus far</p></li>
</ul>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Importing required packages</span>
<span class="kn">from</span> <span class="nn">uuid</span> <span class="kn">import</span> <span class="n">uuid4</span>
<span class="kn">import</span> <span class="nn">tempfile</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>

<span class="c1"># Importing ML packages</span>
<span class="kn">from</span> <span class="nn">pyansys_dqn</span> <span class="kn">import</span> <span class="n">dqn</span><span class="p">,</span> <span class="n">dqn_runner</span><span class="p">,</span> <span class="n">qn_keras</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># For plotting</span>
<span class="kn">from</span> <span class="nn">pyansys_dqn.dqn_runner</span> <span class="kn">import</span> <span class="n">LivePlotter</span>
</pre></div>
</div>
</section>
<section id="native-openai-environment">
<h3>“Native” OpenAI environment<a class="headerlink" href="#native-openai-environment" title="Permalink to this headline">#</a></h3>
<p>Here we take practice swings running reinforcement learning on a known
open source implementation of the classic control problem. You should
see the episode rewards and their running average increase until they
arrive at the training objective (196 for 10-episode average):</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calling OpenAI Cartpole environment and generating folders for backup.</span>
<span class="n">env_name</span> <span class="o">=</span> <span class="s1">&#39;CartPole-v0&#39;</span>
<span class="n">tmp_path</span> <span class="o">=</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">gettempdir</span><span class="p">()</span>

<span class="n">output_path_gym</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tmp_path</span><span class="p">,</span> <span class="s1">&#39;gym_cartpole_results&#39;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">uuid4</span><span class="p">()))</span>
<span class="n">output_name_gym</span> <span class="o">=</span> <span class="s1">&#39;gym_cartpole_00&#39;</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">output_path_gym</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">output_path_gym</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Writing model in: </span><span class="si">{</span><span class="n">output_path_gym</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<pre class="literal-block">Writing model in: C:\Users\gayuso\AppData\Local\Temp\gym_cartpole_results\e22e8cab-251d-4d8a-b698-bcc9ac841ac9</pre>
<p>Now, let’s train the agent.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gym_plotter</span> <span class="o">=</span> <span class="n">LivePlotter</span><span class="p">()</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">dqn_runner</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">env_name</span><span class="p">,</span>
                         <span class="n">dqn</span><span class="o">.</span><span class="n">ClassicDQNLearner</span><span class="p">,</span>
                         <span class="n">qn_keras</span><span class="o">.</span><span class="n">QNetwork</span><span class="p">,</span>
                         <span class="n">layers</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">],</span>
                         <span class="n">n_episodes</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                         <span class="n">epsilon</span><span class="o">=</span><span class="n">dqn_runner</span><span class="o">.</span><span class="n">basic_epsilon_linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mi">1000</span><span class="p">),</span>
                         <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
                         <span class="n">n_mini_batch</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                         <span class="n">replay_db_warmup</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                         <span class="n">replay_db_capacity</span><span class="o">=</span><span class="mi">40000</span><span class="p">,</span>
                         <span class="n">c_cycle</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                         <span class="n">polyak_rate</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
                         <span class="n">averaging_window</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                         <span class="n">victory_threshold</span><span class="o">=</span><span class="mi">196</span><span class="p">,</span>
                         <span class="n">diagnostics_fn</span><span class="o">=</span><span class="n">gym_plotter</span><span class="o">.</span><span class="n">live_plot</span><span class="p">,</span>
                         <span class="n">output_path</span><span class="o">=</span><span class="n">output_path_gym</span><span class="p">,</span>
                         <span class="n">output_name</span><span class="o">=</span><span class="n">output_name_gym</span><span class="p">)</span>
</pre></div>
</div>
<img alt="_images/openai.gif" src="_images/openai.gif" />
</section>
<section id="pyansys-in-openai-environment">
<h3>PyAnsys in OpenAI environment<a class="headerlink" href="#pyansys-in-openai-environment" title="Permalink to this headline">#</a></h3>
<p>Now we do reinforcement learning with MAPDL in the loop as an OpenAI Gym
package, thanks to the PyAnsys API.</p>
<figure class="align-default" id="id6">
<a class="reference internal image-reference" href="_images/ANSYS_loop.jpg"><img alt="Fig 6" src="_images/ANSYS_loop.jpg" style="width: 478.0px; height: 319.0px;" /></a>
<figcaption>
<p><span class="caption-text">Fig 6: A single iteration of the CartPole as a Markov Decision Process using MAPDL in pyansys</span><a class="headerlink" href="#id6" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>In this environment, MAPDL will provide the environment response after
solving.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calling PyAnsys Cartpole environment and generating folders for backup.</span>
<span class="kn">import</span> <span class="nn">pyansys_cartpole</span>

<span class="n">env_name</span> <span class="o">=</span> <span class="s1">&#39;pyansys-CartPole-v0&#39;</span>
<span class="n">tmp_path</span> <span class="o">=</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">gettempdir</span><span class="p">()</span>

<span class="n">output_path_pyansys</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tmp_path</span><span class="p">,</span> <span class="s1">&#39;pyansys_cartpole_results&#39;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">uuid4</span><span class="p">()))</span>
<span class="n">output_name_pyansys</span> <span class="o">=</span> <span class="s1">&#39;pyansys_cartpole_00&#39;</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">output_path_pyansys</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">output_path_pyansys</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Writing model in: </span><span class="si">{</span><span class="n">output_path_pyansys</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<pre class="literal-block">Writing model in: C:\Users\gayuso\AppData\Local\Temp\pyansys_cartpole_results\6c1d4342-1563-4513-a834-9f94bfcc29ab</pre>
<p>We train the learner:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pyansys_plotter</span> <span class="o">=</span> <span class="n">LivePlotter</span><span class="p">()</span>

<span class="n">results2</span> <span class="o">=</span> <span class="n">dqn_runner</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">env_name</span><span class="p">,</span>
                         <span class="n">dqn</span><span class="o">.</span><span class="n">ClassicDQNLearner</span><span class="p">,</span>
                         <span class="n">qn_keras</span><span class="o">.</span><span class="n">QNetwork</span><span class="p">,</span>
                         <span class="n">layers</span><span class="o">=</span><span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">],</span>
                         <span class="n">n_episodes</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span>
                         <span class="n">epsilon</span><span class="o">=</span><span class="n">dqn_runner</span><span class="o">.</span><span class="n">basic_epsilon_linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mi">1000</span><span class="p">),</span>
                         <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
                         <span class="n">n_mini_batch</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                         <span class="n">replay_db_warmup</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                         <span class="n">replay_db_capacity</span><span class="o">=</span><span class="mi">40000</span><span class="p">,</span>
                         <span class="n">c_cycle</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                         <span class="n">polyak_rate</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span>
                         <span class="n">averaging_window</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                         <span class="n">victory_threshold</span><span class="o">=</span><span class="mi">196</span><span class="p">,</span>
                         <span class="n">diagnostics_fn</span><span class="o">=</span><span class="n">pyansys_plotter</span><span class="o">.</span><span class="n">live_plot</span><span class="p">,</span>
                         <span class="n">output_path</span><span class="o">=</span><span class="n">output_path_pyansys</span><span class="p">,</span>
                         <span class="n">output_name</span><span class="o">=</span><span class="n">output_name_pyansys</span><span class="p">)</span>
</pre></div>
</div>
<img alt="_images/pyansys.gif" src="_images/pyansys.gif" />
<p>The learner is trained successfully.</p>
</section>
<section id="random-agent">
<h3>Random Agent<a class="headerlink" href="#random-agent" title="Permalink to this headline">#</a></h3>
<p>For comparison, here we create a simple test agent that behaves randomly
and thus is not likely to succeed at the balancing task.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Importing required packages</span>
<span class="kn">from</span> <span class="nn">pyansys_dqn.test_agents</span> <span class="kn">import</span> <span class="n">RandomAgent</span><span class="p">,</span> <span class="n">TrainedAgent</span>

<span class="n">env_name</span> <span class="o">=</span> <span class="s1">&#39;pyansys-CartPole-v0&#39;</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">env_name</span><span class="p">)</span>

<span class="n">agent</span> <span class="o">=</span> <span class="n">RandomAgent</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">)</span>

<span class="n">s</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Cart position&quot;</span><span class="p">,</span> <span class="s2">&quot;Cart velocity&quot;</span><span class="p">,</span> <span class="s2">&quot;Theta angle&quot;</span><span class="p">,</span> <span class="s2">&quot;Pole velocity&quot;</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot; - &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">each</span><span class="o">.</span><span class="n">center</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> <span class="k">for</span> <span class="n">each</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot; - &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">each</span><span class="si">:</span><span class="s2">6.3</span><span class="si">}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">center</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> <span class="k">for</span> <span class="n">each</span> <span class="ow">in</span> <span class="n">s</span><span class="p">]))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Cart</span> <span class="n">position</span>     <span class="o">-</span>    <span class="n">Cart</span> <span class="n">velocity</span>     <span class="o">-</span>     <span class="n">Theta</span> <span class="n">angle</span>      <span class="o">-</span>    <span class="n">Pole</span> <span class="n">velocity</span>
   <span class="o">-</span><span class="mf">0.00706</span>       <span class="o">-</span>         <span class="mf">0.033</span>        <span class="o">-</span>          <span class="mf">2.18</span>        <span class="o">-</span>           <span class="mf">0.0</span>
</pre></div>
</div>
<p>Below, notice how we inform the agent about each state transition with
<code class="docutils literal notranslate"><span class="pre">agent.start_state(s)</span></code> or <code class="docutils literal notranslate"><span class="pre">agent.next_reading(s,</span> <span class="pre">r,</span> <span class="pre">done)</span></code> and then
ask it to recommend an action with <code class="docutils literal notranslate"><span class="pre">agent.next_action()</span></code>. We inform
the environment this recommendation by feeding the method
<code class="docutils literal notranslate"><span class="pre">env.step(a)</span></code>. We do not expect these recommendations to be good
because this agent selects at random from the choices ‘left’ and
‘right’, with equal probability. A control algorithm that just flips a
coin to select how to behave is usually not effective. Thus, the pole
should not stay balanced for long.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">agent</span><span class="o">.</span><span class="n">start_state</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="n">done</span><span class="p">,</span> <span class="n">r_tot</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="mi">0</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Action - &quot;</span> <span class="o">+</span> <span class="s2">&quot; - &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">each</span><span class="o">.</span><span class="n">center</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> <span class="k">for</span> <span class="n">each</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">]))</span>

<span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">next_action</span><span class="p">()</span>
    <span class="n">s</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">agent</span><span class="o">.</span><span class="n">next_reading</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;---&gt;&#39;</span> <span class="k">if</span> <span class="n">a</span> <span class="k">else</span> <span class="s1">&#39;&lt;--- &#39;</span><span class="p">,</span> <span class="s1">&#39; - &#39;</span><span class="p">,</span> <span class="s2">&quot; - &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">each</span><span class="si">:</span><span class="s2">6.3</span><span class="si">}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">center</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> <span class="k">for</span> <span class="n">each</span> <span class="ow">in</span> <span class="n">s</span><span class="p">]))</span>
    <span class="n">r_tot</span> <span class="o">+=</span> <span class="n">r</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Total timesteps:&#39;</span><span class="p">,</span> <span class="n">r_tot</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Final theta angle: </span><span class="si">{</span><span class="n">s</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">:</span><span class="s1">4.2f</span><span class="si">}</span><span class="s1"> degrees&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Action</span> <span class="o">-</span>    <span class="n">Cart</span> <span class="n">position</span>     <span class="o">-</span>    <span class="n">Cart</span> <span class="n">velocity</span>     <span class="o">-</span>     <span class="n">Theta</span> <span class="n">angle</span>      <span class="o">-</span>    <span class="n">Pole</span> <span class="n">velocity</span>
<span class="o">---&gt;</span>  <span class="o">-</span>        <span class="o">-</span><span class="mf">0.0214</span>        <span class="o">-</span>        <span class="mf">0.0491</span>        <span class="o">-</span>        <span class="mf">0.0498</span>        <span class="o">-</span>          <span class="mf">3.29</span>

<span class="n">Total</span> <span class="n">timesteps</span><span class="p">:</span> <span class="mi">2</span>
<span class="n">Final</span> <span class="n">theta</span> <span class="n">angle</span><span class="p">:</span> <span class="mf">0.05</span> <span class="n">degrees</span>
</pre></div>
</div>
<p>As we see the theta in the last reported state (last row, third column),
theta exceed the 12 degrees maximum, hence the simulation is stopped.</p>
</section>
</section>
<section id="reusing-trained-agent">
<h2>Reusing trained agent<a class="headerlink" href="#reusing-trained-agent" title="Permalink to this headline">#</a></h2>
<p>Now we create an agent that has been trained, i.e., that refers to a
successful neural networks in order to decide how best to act. It is
thus much more likely to perform well and balance the pole for a
noticeably greater number of steps… all this despite having a random
starting point for the system!</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_actions</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">TrainedAgent</span><span class="p">(</span><span class="n">output_path_pyansys</span><span class="p">,</span> <span class="n">output_name_pyansys</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">s</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot; - &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">each</span><span class="o">.</span><span class="n">center</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> <span class="k">for</span> <span class="n">each</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot; - &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">each</span><span class="si">:</span><span class="s2">6.3</span><span class="si">}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">center</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> <span class="k">for</span> <span class="n">each</span> <span class="ow">in</span> <span class="n">s</span><span class="p">]))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Cart</span> <span class="n">position</span>     <span class="o">-</span>    <span class="n">Cart</span> <span class="n">velocity</span>     <span class="o">-</span>     <span class="n">Theta</span> <span class="n">angle</span>      <span class="o">-</span>    <span class="n">Pole</span> <span class="n">velocity</span>
    <span class="o">-</span><span class="mf">0.039</span>        <span class="o">-</span>        <span class="mf">0.0487</span>        <span class="o">-</span>        <span class="o">-</span><span class="mf">0.569</span>        <span class="o">-</span>           <span class="mf">0.0</span>
</pre></div>
</div>
<p>Below, notice how we inform the agent about each state transition with
<code class="docutils literal notranslate"><span class="pre">agent.start_state(s)</span></code> or <code class="docutils literal notranslate"><span class="pre">agent.next_reading(s,</span> <span class="pre">r,</span> <span class="pre">done)</span></code> and then
ask it to recommend an action with <code class="docutils literal notranslate"><span class="pre">agent.next_action()</span></code>. We follow
its recommendation by feeding it into the environment in
<code class="docutils literal notranslate"><span class="pre">env.step(a)</span></code>. The recommendations should be pretty good because they
stem from neural networks that store the information resulting from
successful training and the pole should stay up longer, hopefully for
the entirety of the episode (200 steps).</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">agent</span><span class="o">.</span><span class="n">start_state</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="n">done</span><span class="p">,</span> <span class="n">r_tot</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="mi">0</span>

<span class="c1"># print(&quot;Action - &quot; + &quot; - &quot;.join([each.center(20) for each in labels])) # Uncomment to print each step</span>

<span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">next_action</span><span class="p">()</span>
    <span class="n">s</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">agent</span><span class="o">.</span><span class="n">next_reading</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="c1"># print(&#39;---&gt; &#39; if a else &#39;&lt;--- &#39;, &#39; - &#39;, &quot; - &quot;.join([ f&quot;{each:6.3}&quot;.center(20) for each in s])) # Uncomment to print each step</span>
    <span class="n">r_tot</span> <span class="o">+=</span> <span class="n">r</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot; - &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">each</span><span class="o">.</span><span class="n">center</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> <span class="k">for</span> <span class="n">each</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot; - &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">each</span><span class="si">:</span><span class="s2">6.3</span><span class="si">}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">center</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> <span class="k">for</span> <span class="n">each</span> <span class="ow">in</span> <span class="n">s</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Total timesteps:&#39;</span><span class="p">,</span> <span class="n">r_tot</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Final theta angle: </span><span class="si">{</span><span class="n">s</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">:</span><span class="s1">4.2f</span><span class="si">}</span><span class="s1"> degrees&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>   <span class="n">Cart</span> <span class="n">position</span>     <span class="o">-</span>    <span class="n">Cart</span> <span class="n">velocity</span>     <span class="o">-</span>     <span class="n">Theta</span> <span class="n">angle</span>      <span class="o">-</span>    <span class="n">Pole</span> <span class="n">velocity</span>
      <span class="o">-</span><span class="mf">0.0392</span>        <span class="o">-</span>         <span class="mf">0.105</span>        <span class="o">-</span>        <span class="o">-</span><span class="mf">0.464</span>        <span class="o">-</span>         <span class="mf">0.549</span>

<span class="n">Total</span> <span class="n">timesteps</span><span class="p">:</span> <span class="mi">201</span>
<span class="n">Final</span> <span class="n">theta</span> <span class="n">angle</span><span class="p">:</span> <span class="o">-</span><span class="mf">0.46</span> <span class="n">degrees</span>
</pre></div>
</div>
<p>We are just printing the last time step.</p>
</section>
<section id="epilogue">
<h2>Epilogue<a class="headerlink" href="#epilogue" title="Permalink to this headline">#</a></h2>
<p>Try resuming a trained neural network of your own!</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://www.tensorflow.org/agents/tutorials/0_intro_rl#the_dqn_agent">The DQN Agent - TensofFlow.org</a></p></li>
<li><p><a class="reference external" href="https://www.nature.com/articles/nature14236">Human-level control through deep reinforcement learning - V Mnih et al.</a></p></li>
</ul>
</section>
</section>


                </article>
              
              
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#background-markov-decision-process">Background: Markov Decision Process</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cartpole-simulation">CartPole Simulation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mapdl-in-the-loop">MAPDL in the loop</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#instance-creation-mapdl-in-the-loop">Instance creation: MAPDL in the loop</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reinforcement-learning-deep-q-network">Reinforcement Learning: Deep Q-Network</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-network-parameters">Neural Network Parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-task">Learning Task</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#native-openai-environment">“Native” OpenAI environment</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pyansys-in-openai-environment">PyAnsys in OpenAI environment</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random-agent">Random Agent</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reusing-trained-agent">Reusing trained agent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#epilogue">Epilogue</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">
  <div class="tocsection sourcelink">
    <a href="_sources/ml-rl-notebook.rst.txt">
      <i class="fa-solid fa-file-lines"></i> Show Source
    </a>
  </div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
  <p class="copyright">
    
      © Copyright (c) 2023 ANSYS, Inc. All rights reserved.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">
  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 4.5.0.
    <br/>
  </p>
</div>
      
    </div>
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><p class="theme-version">
    Built with the <a href="https://sphinxdocs.ansys.com/version/stable/index.html">Ansys Sphinx Theme</a> 0.9.9.
    <br>Last updated on <span id="date"></span></br>
</p>
<script>
  var options = { day: 'numeric', month: 'long', year: 'numeric' };
  var lastModifiedDate = new Date(document.lastModified);
  var date = lastModifiedDate.toLocaleDateString('en-US', options);
  document.getElementById("date").innerHTML = date;
</script></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>