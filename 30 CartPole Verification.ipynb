{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole Test   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./pyansys_rl\n",
      "Processing ./pyansys_gym\n",
      "Requirement already satisfied: gym==0.10.9 in /home/jovyan/.local/lib/python3.7/site-packages (from pyansys-rl==0.0.1) (0.10.9)\n",
      "Requirement already satisfied: keras==2.2.4 in /home/jovyan/.local/lib/python3.7/site-packages (from pyansys-rl==0.0.1) (2.2.4)\n",
      "Requirement already satisfied: numpy>=1.16 in /opt/conda/lib/python3.7/site-packages (from pyansys-rl==0.0.1) (1.18.4)\n",
      "Requirement already satisfied: tensorflow==1.13.1 in /home/jovyan/.local/lib/python3.7/site-packages (from pyansys-rl==0.0.1) (1.13.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from gym==0.10.9->pyansys-rl==0.0.1) (1.4.1)\n",
      "Requirement already satisfied: requests>=2.0 in /opt/conda/lib/python3.7/site-packages (from gym==0.10.9->pyansys-rl==0.0.1) (2.23.0)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym==0.10.9->pyansys-rl==0.0.1) (1.5.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gym==0.10.9->pyansys-rl==0.0.1) (1.15.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from keras==2.2.4->pyansys-rl==0.0.1) (5.3.1)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras==2.2.4->pyansys-rl==0.0.1) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from keras==2.2.4->pyansys-rl==0.0.1) (1.1.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/jovyan/.local/lib/python3.7/site-packages (from keras==2.2.4->pyansys-rl==0.0.1) (1.0.8)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1->pyansys-rl==0.0.1) (0.11.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1->pyansys-rl==0.0.1) (0.3.3)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1->pyansys-rl==0.0.1) (0.34.2)\n",
      "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /home/jovyan/.local/lib/python3.7/site-packages (from tensorflow==1.13.1->pyansys-rl==0.0.1) (1.13.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/jovyan/.local/lib/python3.7/site-packages (from tensorflow==1.13.1->pyansys-rl==0.0.1) (0.8.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1->pyansys-rl==0.0.1) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/jovyan/.local/lib/python3.7/site-packages (from tensorflow==1.13.1->pyansys-rl==0.0.1) (1.33.1)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /home/jovyan/.local/lib/python3.7/site-packages (from tensorflow==1.13.1->pyansys-rl==0.0.1) (1.13.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1->pyansys-rl==0.0.1) (3.13.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0->gym==0.10.9->pyansys-rl==0.0.1) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0->gym==0.10.9->pyansys-rl==0.0.1) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0->gym==0.10.9->pyansys-rl==0.0.1) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0->gym==0.10.9->pyansys-rl==0.0.1) (1.25.9)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from pyglet>=1.2.0->gym==0.10.9->pyansys-rl==0.0.1) (0.18.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1->pyansys-rl==0.0.1) (3.3.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1->pyansys-rl==0.0.1) (1.0.1)\n",
      "Requirement already satisfied: mock>=2.0.0 in /home/jovyan/.local/lib/python3.7/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1->pyansys-rl==0.0.1) (4.0.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow==1.13.1->pyansys-rl==0.0.1) (46.4.0.post20200518)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1->pyansys-rl==0.0.1) (1.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1->pyansys-rl==0.0.1) (3.1.0)\n",
      "Building wheels for collected packages: pyansys-rl, pyansys-cartpole\n",
      "  Building wheel for pyansys-rl (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyansys-rl: filename=pyansys_rl-0.0.1-py3-none-any.whl size=7071 sha256=a9bd0f5c7a73eef1119ce50245e43dd6ac2fcd5784001965d402f51291953f00\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-eneeb3gv/wheels/ab/f3/ff/bebff563baa460ac7ab74ef83360389d4ae6f5a05651e162f3\n",
      "  Building wheel for pyansys-cartpole (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyansys-cartpole: filename=pyansys_cartpole-0.0.1-py3-none-any.whl size=10366 sha256=c4c078ac3120cf716c7ca521eb5e3d02e54f928a3f5b520de338cca7fd589076\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-eneeb3gv/wheels/80/64/72/e71acde093f23c47c6e87f82f5841c0c889c0fcfdb32351491\n",
      "Successfully built pyansys-rl pyansys-cartpole\n",
      "Installing collected packages: pyansys-rl, pyansys-cartpole\n",
      "  Attempting uninstall: pyansys-rl\n",
      "    Found existing installation: pyansys-rl 0.0.1\n",
      "    Uninstalling pyansys-rl-0.0.1:\n",
      "      Successfully uninstalled pyansys-rl-0.0.1\n",
      "  Attempting uninstall: pyansys-cartpole\n",
      "    Found existing installation: pyansys-cartpole 0.0.1\n",
      "    Uninstalling pyansys-cartpole-0.0.1:\n",
      "      Successfully uninstalled pyansys-cartpole-0.0.1\n",
      "Successfully installed pyansys-cartpole-0.0.1 pyansys-rl-0.0.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install ./pyansys_rl ./pyansys_gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import pyansys_cartpole\n",
    "from pyansys_dqn import dqn, dqn_runner, qn_keras\n",
    "from pyansys_dqn.test_agents import RandomAgent, TrainedAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gayuso\\AppData\\Roaming\\Python\\Python38\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env_name = 'pyansys-CartPole-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Agent\n",
    "Here we create a simple test agent that behaves randomly and thus is not likely to succeed at the balancing task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RandomAgent(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0328 -0.0433 -2.7321  0.    ]\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, notice how we inform the agent about each state transition with `agent.start_state(s)` or `agent.next_reading(s, r, done)` and then ask it to recommend an action with `agent.next_action()`.  We inform the environment this recommendation by feeding the method `env.step(a)`.  We do not expect these recommendations to be good because this agent selects at random from the choices 'left' and 'right', with equal probability.  A control algorithm that just flips a coin to select how to behave is usually not effective.  Thus, the pole should not stay balanced for long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<--- [-0.0338 -0.1072 -2.6191  0.0232]\n",
      "<--- [-0.0379 -0.3012 -2.2815  0.0957]\n",
      "total timesteps: 2\n"
     ]
    }
   ],
   "source": [
    "agent.start_state(s)\n",
    "done, r_tot = False, 0\n",
    "while not done:\n",
    "    a = agent.next_action()\n",
    "    s, r, done, _ = env.step(a)\n",
    "    print('--->' if a else '<---', s)\n",
    "    agent.next_reading(s, r, done, False)\n",
    "    r_tot += r\n",
    "print('total timesteps:', r_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained Agent\n",
    "Now we create an agent that has been trained, i.e., that refers to a successful neural networks in order to decide how best to act. It is thus much more likely to perform well and balance the pole for a noticeably greater number of steps... all this despite having a random starting point for the system!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path(os.getcwd(), 'successful_runs','pyansys_cartpole')\n",
    "output_name = 'pyansys_cartpole_00'\n",
    "n_actions = 2\n",
    "agent = TrainedAgent(output_path, output_name, env.action_space.n, env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0242 -0.0313 -0.8666  0.    ]\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, notice how we inform the agent about each state transition with `agent.start_state(s)` or `agent.next_reading(s, r, done)` and then ask it to recommend an action with `agent.next_action()`.  We follow its recommendation by feeding it into the environment in `env.step(a)`.  The recommendations should be pretty good because they stem from neural networks that store the information resulting from successful training and the pole should stay up longer, hopefully for the entirety of the episode (200 steps). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<--- [-0.0253 -0.1075 -0.7492  0.0272]\n",
      "<--- [-0.0294 -0.3019 -0.3988  0.1091]\n",
      "total timesteps: 2\n"
     ]
    }
   ],
   "source": [
    "agent.start_state(s)\n",
    "done, r_tot = False, 0\n",
    "while not done:\n",
    "    a = agent.next_action()\n",
    "    s, r, done, _ = env.step(a)\n",
    "    print('--->' if a else '<---', s)\n",
    "    agent.next_reading(s, r, done, False)\n",
    "    r_tot += r\n",
    "print('total timesteps:', r_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epilogue\n",
    "Try resuming a trained neural network of your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
