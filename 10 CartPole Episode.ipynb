{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Episodes of CartPole implemented in MAPDL   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install numpy\n",
    "#!pip install gym\n",
    "\n",
    "#!pip install ./pyansys_rl ./pyansys_gym -q --user --no-warn-script-location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import pyansys_cartpole\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Background:  Markov Decision Process</h2>\n",
    "<img src=\"media/MDP_board.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Markov Decision Process we have an agent immersed in an environment.  At any given time, the agent finds itself in a state and it must select one of the available actions.  Upon taking an action, the environment reponds by assigning a reward and transitioning the agent to a successor state.  This loop continues until a terminal state is reached.  It is interesting to ask: could we learn to act optimally in such a setup? could we learn to select sequences of actions that maximize long term cumulative rewards? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"media/MDP_loop.jpg\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>CartPole</h2>\n",
    "<img src=\"media/cartpole_description.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CartPole is a classic control problem.  It is a balancing task: push the cart such that the pinned pole remains upright. In other words, the pole behaves as a solid inverted pendulum and is unstable about the desired configuration.  A simple implementation could use a revolute/hinge joint between the cart and the pole, and a translational joint between the cart and the ground. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>MAPDL in the loop</h3>\n",
    "<img src=\"media/ANSYS_loop.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<center>Fig: A single iteration of the CartPole as a Markov Decision Process using MAPDL</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this implementation of the CartPole as a Markov Decision Process, we highlight the following components:\n",
    "\n",
    "* Actions: push either left (0) or right (1) \n",
    "* State: $x_{\\text{cart}}, v_{\\text{cart}}, \\theta_{\\text{pole}}, v_{\\text{pole}}$\n",
    "* Reward: +1 for every timestep still in equilibrium\n",
    "* Transition Model: courtesy of an MAPDL structural transient analysis \n",
    "\n",
    "At each episode, the system starts in a randomly seeded state, with positions, velocities and angles picked from a uniform distribution about the vertical/resting position, thus it is unlikely to ever be at equilibrium.  Even if it were, the equilibrium would be unstable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance creation: MAPDL in the loop\n",
    "Create an instance of an MAPDL environment that is specially wrapped for use in [OpenAI Gym](https://gym.openai.com/) thanks to the newly developed python gRPC bindings ([pyansys](https://pypi.org/project/pyansys/)).  The wrapper sets up the CartPole physics, accepts the available actions (i.e. forces), and calculates the state transitions (kinematic response) after every time step (an MAPDL load step).  For reference, OpenAI Gym provides its own ad hoc [environment](https://gym.openai.com/envs/CartPole-v1/) for solving the system's [kinematic equations](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gayuso\\AppData\\Roaming\\Python\\Python38\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env_name = 'pyansys-CartPole-v0'\n",
    "env = gym.make(env_name)\n",
    "mapdl = env.env.env._mapdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run several episodes (e.g., 3) of the CartPole using a random action, i.e., sometimes 0 (push left), sometimes 1 (push right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Episode: 1 ******************************\n",
      "State: [0.0472 0.0043 2.2449 0.    ] \tAction: ---> \tReward:  1\n",
      "State: [0.0483 0.1073 2.1307 0.0242] \tAction: ---> \tReward:  1\n",
      "Episode Reward: 2\n",
      "\n",
      "****************************** Episode: 2 ******************************\n",
      "State: [ 0.0143 -0.0053 -0.2162  0.    ] \tAction: <--- \tReward:  1\n",
      "State: [ 0.0132 -0.1076 -0.0974  0.0288] \tAction: ---> \tReward:  1\n",
      "Episode Reward: 2\n",
      "\n",
      "****************************** Episode: 3 ******************************\n",
      "State: [-0.034  -0.0353  2.7568  0.    ] \tAction: <--- \tReward:  1\n",
      "State: [-0.0351 -0.108   2.8821  0.0363] \tAction: ---> \tReward:  1\n",
      "Episode Reward: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 3\n",
    "for i in range(n_episodes):\n",
    "    print('*' * 30, f'Episode: {i+1}', '*' * 30)\n",
    "    cur_state = env.reset()\n",
    "    done, r_tot = False, 0\n",
    "    while not done:\n",
    "        action = np.random.choice([0, 1])\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        print('State:', cur_state, '\\tAction:', '--->' if action else '<---', '\\tReward: ', reward)\n",
    "        cur_state, r_tot = next_state, r_tot + reward\n",
    "    print('Episode Reward:', r_tot)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
