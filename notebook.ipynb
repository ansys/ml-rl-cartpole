{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Machine Learning using PyMAPDL: Cartpole Simulation\n",
    "\n",
    "This notebook demonstrates a reinforcement machine learning example using MAPDL through PyMAPDL.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is using lazily downloaded wheels using HTTP range requests to obtain dependency information. This experimental feature is enabled through --use-feature=fast-deps and it is not ready for production.\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Installing pyansys packages\n",
    "!pip install ./pyansys_rl ./pyansys_gym -q --use-feature=fast-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading requires packages\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "import gymnasium as gym\n",
    "import pyansys_cartpole\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background:  Markov Decision Process\n",
    "\n",
    "\n",
    "![Fig 1: Learning agent.](media/MDP_board.jpg \"Fig 1: Learning agent.\")\n",
    "\n",
    "**Fig 1: Learning agent.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Markov Decision Process we have an agent immersed in an environment.  At any given time, the agent finds itself in a state and it must select one of the available actions.  Upon taking an action, the environment reponds by assigning a reward and transitioning the agent to a successor state.  This loop continues until a terminal state is reached.  It is interesting to ask: could we learn to act optimally in such a setup? could we learn to select sequences of actions that maximize long term cumulative rewards?\n",
    "\n",
    "![Fig 2: Markov Decision Process in Reinforced Learning](media/MDP_loop.jpg \"Fig 2: Markov Decision Process in Reinforced Learning\")\n",
    "\n",
    "**Fig 2: Markov Decision Process in Reinforced Learning**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole Simulation\n",
    "\n",
    "The CartPole is a classic control problem.  It is a balancing task: push the cart such that the pinned pole remains upright. In other words, the pole behaves as a solid inverted pendulum and is unstable about the desired configuration.  A simple implementation could use a revolute/hinge joint between the cart and the pole, and a translational joint between the cart and the ground.\n",
    "\n",
    "![Fig 3: Cartpole problem.](media/cartpole_description.jpg \"Fig 3: Cartpole problem.\")\n",
    "\n",
    "**Fig 3: Cartpole problem.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAPDL in the loop\n",
    "\n",
    "In this implementation of the CartPole as a Markov Decision Process, we highlight the following components:\n",
    "\n",
    "* Actions: push either left ($action = 0$) or right ($action = 1$) \n",
    "* State: $x_{\\text{cart}}, v_{\\text{cart}}, \\theta_{\\text{pole}}, v_{\\text{pole}}$\n",
    "* Reward: +1 for every timestep still in equilibrium\n",
    "* Transition Model: courtesy of an MAPDL structural transient analysis \n",
    "\n",
    "At each episode, the system starts in a randomly seeded state, with positions, velocities and angles picked from a uniform distribution about the vertical/resting position, thus it is unlikely to ever be at equilibrium.  Even if it were, the equilibrium would be unstable. \n",
    "\n",
    "\n",
    "![Fig 4: A single iteration of the CartPole as a Markov Decision Process using MAPDL](media/ANSYS_loop.jpg \"Fig 4: A single iteration of the CartPole as a Markov Decision Process using MAPDL\")\n",
    "\n",
    "**Fig 4: A single iteration of the CartPole as a Markov Decision Process using MAPDL**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance creation: MAPDL in the loop\n",
    "Create an instance of an MAPDL environment that is specially wrapped for use in [OpenAI Gym](https://gym.openai.com/) thanks to the newly developed python gRPC bindings ([PyAnsys](https://github.com/pyansys)).  The wrapper sets up the CartPole physics, accepts the available actions (i.e. forces), and calculates the state transitions (kinematic response) after every time step (an MAPDL load step).\n",
    "\n",
    "For reference, OpenAI Gym provides its own ad hoc [environment](https://gym.openai.com/envs/CartPole-v1/) for solving the system's [kinematic equations](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\gymnasium\\envs\\registration.py:435: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes']\u001b[0m\n",
      "  logger.warn(\n",
      "d:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gymnasium versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "d:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:203: UserWarning: \u001b[33mWARN: Future gymnasium versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Episode: 1 ******************************\n",
      "State: [ 0.0106 -0.0048  0.2215  0.    ] \tAction: ---> \tReward:  1\n",
      "State: [ 0.0117  0.1073 -0.3949  0.0361] \tAction: ---> \tReward:  1\n",
      "State: [ 0.0158  0.3024 -2.7399  0.1067] \tAction: <--- \tReward:  1\n",
      "State: [ 0.0217  0.283  -6.0686  0.1028] \tAction: <--- \tReward:  1\n",
      "State: [ 0.0254  0.0882 -8.1605  0.036 ] \tAction: <--- \tReward:  1\n",
      "State: [ 0.0252 -0.1067 -8.0565  0.0248] \tAction: <--- \tReward:  1\n",
      "State: [ 0.0211 -0.3016 -5.7533  0.0914] \tAction: ---> \tReward:  1\n",
      "State: [ 0.0153 -0.2819 -2.4326  0.0826] \tAction: ---> \tReward:  1\n",
      "State: [ 0.0116 -0.0869 -0.3213  0.0111] \tAction: ---> \tReward:  1\n",
      "State: [ 0.0118  0.1083 -0.444   0.054 ] \tAction: ---> \tReward:  1\n",
      "State: [ 0.0159  0.3034 -2.8002  0.1259] \tAction: <--- \tReward:  1\n",
      "State: [ 0.0218  0.284  -6.1398  0.1221] \tAction: ---> \tReward:  1\n",
      "State: [ 0.0277  0.304  -9.4453  0.1299] \tAction: ---> \tReward:  1\n",
      "Episode Reward: 13\n",
      "\n",
      "****************************** Episode: 2 ******************************\n",
      "State: [ 0.0119  0.0381 -2.2843  0.    ] \tAction: <--- \tReward:  1\n",
      "State: [ 0.0109 -0.107  -1.6696  0.0311] \tAction: <--- \tReward:  1\n",
      "State: [ 0.0068 -0.3016  0.669   0.0921] \tAction: <--- \tReward:  1\n",
      "State: [-0.0012 -0.4964  5.2007  0.1513] \tAction: ---> \tReward:  1\n",
      "State: [-0.0109 -0.4766 10.5941  0.1437] \tAction: <--- \tReward:  1\n",
      "Episode Reward: 5\n",
      "\n",
      "****************************** Episode: 3 ******************************\n",
      "State: [-0.0451  0.0352 -0.6833  0.    ] \tAction: <--- \tReward:  1\n",
      "State: [-0.0462 -0.1073 -0.0673  0.035 ] \tAction: ---> \tReward:  1\n",
      "State: [-0.0481 -0.0875  1.0453  0.0299] \tAction: <--- \tReward:  1\n",
      "State: [-0.0501 -0.1072  2.1617  0.0272] \tAction: <--- \tReward:  1\n",
      "State: [-0.0542 -0.302   4.4944  0.104 ] \tAction: ---> \tReward:  1\n",
      "State: [-0.06   -0.2827  7.7936  0.094 ] \tAction: ---> \tReward:  1\n",
      "State: [-0.0637 -0.0878  9.8582  0.0299] \tAction: ---> \tReward:  1\n",
      "State: [-0.0635  0.1072  9.7508  0.0332] \tAction: ---> \tReward:  1\n",
      "State: [-0.0594  0.302   7.4677  0.1007] \tAction: <--- \tReward:  1\n",
      "State: [-0.0536  0.2825  4.1652  0.0914] \tAction: <--- \tReward:  1\n",
      "State: [-0.0499  0.0874  2.0552  0.0234] \tAction: <--- \tReward:  1\n",
      "State: [-0.0501 -0.1076  2.1708  0.0439] \tAction: ---> \tReward:  1\n",
      "State: [-0.052  -0.088   3.2851  0.0378] \tAction: <--- \tReward:  1\n",
      "State: [-0.054  -0.1077  4.4025  0.0395] \tAction: <--- \tReward:  1\n",
      "State: [-0.0581 -0.3027  6.726   0.1157] \tAction: <--- \tReward:  1\n",
      "State: [-0.0661 -0.4982 11.1838  0.1832] \tAction: ---> \tReward:  1\n",
      "Episode Reward: 16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading environment\n",
    "env_name = 'pyansys-CartPole-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# run several episodes (e.g., 3) of the CartPole using a random action, i.e., sometimes 0 (push left), sometimes 1 (push right)\n",
    "n_episodes = 3\n",
    "for i in range(n_episodes):\n",
    "    print('*' * 30, f'Episode: {i+1}', '*' * 30)\n",
    "    cur_state, info = env.reset()\n",
    "    done, r_tot = False, 0\n",
    "    while not done:\n",
    "        action = np.random.choice([0, 1])\n",
    "        # print(action)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        # print(next_state, reward, terminated, truncated, info)\n",
    "        done = terminated or truncated\n",
    "        print('State:', cur_state, '\\tAction:', '--->' if action else '<---', '\\tReward: ', reward)\n",
    "        cur_state, r_tot = next_state, r_tot + reward\n",
    "    print('Episode Reward:', r_tot)\n",
    "    print('')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results express the actions taken by the agent and the correpondent rewards and states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning: Deep Q-Network\n",
    "\n",
    "The Deep Q-Network (DQN) implementation is inspired by the famous paper [Mnih et al, 2014](https://www.nature.com/articles/nature14236).\n",
    "\n",
    "The algorithm consists of two neural networks (NN) to accumulate the statistical information about optimal reward sequences. \n",
    "One NN, used for learning, is trained at every step by picking a random sample of experiences (state, action, reward)-tuples from an \"experience buffer\".  \n",
    "The buffer is filled from new experiences (old experiences are retired once the contents exceed capacity) as the agent trains.  \n",
    "At the outset, the buffer is pre-filled with experiences from an agent that behaves at random for a fixed number of steps.  \n",
    "\n",
    "\n",
    "![Fig 5: Learning buffer.](media/DQN_clean.jpg \"Fig 5: Learning buffer.\")\n",
    "\n",
    "**Fig 5: Learning buffer.**\n",
    "\n",
    "\n",
    "The other NN, used for behavior, is just a snapshot of the first refreshed at fixed intervals and dictates what actions to take. \n",
    "\n",
    "### Neural Network Parameters\n",
    "\n",
    "* Neural network size/topology:  [4, 32, 32, 2]\n",
    "    * 4: for each components of the state ($x_{\\text{cart}}, v_{\\text{cart}}, \\theta_{\\text{pole}}, v_{\\text{pole}}$)\n",
    "    * 32: hidden layer 1 with 32 ReLU neurons\n",
    "    * 32: hidden layer 2 with 32 ReLU neurons\n",
    "    * 2: for each possible action with linear neurons [Left, Right]\n",
    "* Neural network refresh rate: every timestep\n",
    "* Buffer capacity: 40K experiences\n",
    "* Buffer warmup: 1K experiences\n",
    "* Sample size: 64 experiences per timestep\n",
    "* $\\varepsilon$: linear decay from 1 to .05 in 1000 timestep (.05 thereafter)\n",
    "* $\\gamma = .99$\n",
    "\n",
    "### Learning Task\n",
    "Try to balance the pole up to 200 steps.  Declare success when the 10-episode average exceeds 196\n",
    "\n",
    "Each learner will display a history of performance per episode, encoded as follows:\n",
    "\n",
    "* e: episode number\n",
    "* n: timesteps \n",
    "* x: max reward thus far\n",
    "* r: reward\n",
    "* v: avg reward (e.g., for last 10 steps)\n",
    "* vx: max average reward\n",
    "* nt: total timesteps thus far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required packages\n",
    "from uuid import uuid4\n",
    "import tempfile\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Importing ML packages\n",
    "from pyansys_dqn import dqn, dqn_runner, qn_keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# For plotting\n",
    "from pyansys_dqn.dqn_runner import LivePlotter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Native\" OpenAI environment\n",
    "\n",
    "Here we take practice swings running reinforcement learning on a known open source implementation of the classic control problem.\n",
    "You should see the episode rewards and their running average increase until they arrive at the training objective (196 for 10-episode average):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model in: C:\\Users\\clatapie\\AppData\\Local\\Temp\\gym_cartpole_results\\5a39de26-3d40-4a68-83b1-a8f80584a83e\n"
     ]
    }
   ],
   "source": [
    "# Calling OpenAI Cartpole environment and generating folders for backup.\n",
    "env_name = 'CartPole-v1'\n",
    "tmp_path = tempfile.gettempdir()\n",
    "\n",
    "output_path_gym = os.path.join(tmp_path, 'gym_cartpole_results', str(uuid4())) \n",
    "output_name_gym = 'gym_cartpole_00'\n",
    "\n",
    "if not os.path.exists(output_path_gym):\n",
    "    os.makedirs(output_path_gym)\n",
    "print(f'Writing model in: {output_path_gym}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001E0FB11F010> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 924us/step\n",
      "2/2 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"d:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"d:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"d:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1027, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"d:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 527, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"d:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 1140, in apply_gradients\n        return super().apply_gradients(grads_and_vars, name=name)\n    File \"d:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 634, in apply_gradients\n        iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"d:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 1166, in _internal_apply_gradients\n        return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"d:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 1216, in _distributed_apply_gradients_fn\n        distribution.extended.update(\n    File \"d:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 1213, in apply_grad_to_update_var  **\n        return self._update_step(grad, var)\n    File \"d:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 224, in _update_step\n        self.update_step(gradient, variable)\n    File \"d:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\adam.py\", line 200, in update_step\n        variable.assign_sub((m * alpha) / (tf.sqrt(v) + self.epsilon))\n\n    ValueError: None values not supported.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m gym_plotter \u001b[39m=\u001b[39m LivePlotter()\n\u001b[1;32m----> 3\u001b[0m results \u001b[39m=\u001b[39m dqn_runner\u001b[39m.\u001b[39;49mrun(env_name,\n\u001b[0;32m      4\u001b[0m                          dqn\u001b[39m.\u001b[39;49mClassicDQNLearner,\n\u001b[0;32m      5\u001b[0m                          qn_keras\u001b[39m.\u001b[39;49mQNetwork,\n\u001b[0;32m      6\u001b[0m                          layers\u001b[39m=\u001b[39;49m[\u001b[39m64\u001b[39;49m, \u001b[39m64\u001b[39;49m],\n\u001b[0;32m      7\u001b[0m                          n_episodes\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m,\n\u001b[0;32m      8\u001b[0m                          epsilon\u001b[39m=\u001b[39;49mdqn_runner\u001b[39m.\u001b[39;49mbasic_epsilon_linear(\u001b[39m1\u001b[39;49m, \u001b[39m0.05\u001b[39;49m, \u001b[39m1000\u001b[39;49m),\n\u001b[0;32m      9\u001b[0m                          gamma\u001b[39m=\u001b[39;49m\u001b[39m0.99\u001b[39;49m,\n\u001b[0;32m     10\u001b[0m                          n_mini_batch\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,\n\u001b[0;32m     11\u001b[0m                          replay_db_warmup\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m,\n\u001b[0;32m     12\u001b[0m                          replay_db_capacity\u001b[39m=\u001b[39;49m\u001b[39m40000\u001b[39;49m,\n\u001b[0;32m     13\u001b[0m                          c_cycle\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     14\u001b[0m                          polyak_rate\u001b[39m=\u001b[39;49m\u001b[39m0.99\u001b[39;49m,\n\u001b[0;32m     15\u001b[0m                          averaging_window\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[0;32m     16\u001b[0m                          victory_threshold\u001b[39m=\u001b[39;49m\u001b[39m196\u001b[39;49m,\n\u001b[0;32m     17\u001b[0m                          diagnostics_fn\u001b[39m=\u001b[39;49mgym_plotter\u001b[39m.\u001b[39;49mlive_plot,\n\u001b[0;32m     18\u001b[0m                          output_path\u001b[39m=\u001b[39;49moutput_path_gym,\n\u001b[0;32m     19\u001b[0m                          output_name\u001b[39m=\u001b[39;49moutput_name_gym)\n",
      "File \u001b[1;32md:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\pyansys_dqn\\dqn_runner.py:102\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(env_name, learner_factory, regression_factory, layers, n_episodes, epsilon, gamma, n_mini_batch, replay_db_warmup, replay_db_capacity, c_cycle, polyak_rate, averaging_window, victory_threshold, double_dqn, potential_fn, diagnostics_fn, output_path, output_name)\u001b[0m\n\u001b[0;32m    100\u001b[0m sp, r, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(a)\n\u001b[0;32m    101\u001b[0m done \u001b[39m=\u001b[39m truncated \u001b[39mor\u001b[39;00m terminated\n\u001b[1;32m--> 102\u001b[0m learner\u001b[39m.\u001b[39;49mnext_reading(sp, r, done)\n\u001b[0;32m    103\u001b[0m r_tot \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m r\n\u001b[0;32m    104\u001b[0m steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32md:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\pyansys_dqn\\dqn.py:235\u001b[0m, in \u001b[0;36mClassicDQNLearner.next_reading\u001b[1;34m(self, sp, r, done, train)\u001b[0m\n\u001b[0;32m    232\u001b[0m y[np\u001b[39m.\u001b[39marange(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_mini_batch), actions] \u001b[39m=\u001b[39m temporal_diff\n\u001b[0;32m    234\u001b[0m \u001b[39m# perform a gradient descent step on (y - Q(s_j, a_j, theta))^2 with respect to parameters theta\u001b[39;00m\n\u001b[1;32m--> 235\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq\u001b[39m.\u001b[39;49mfit_batch(states, y)\n\u001b[0;32m    236\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    238\u001b[0m \u001b[39m# every C steps, reset Q_hat <- Q\u001b[39;00m\n",
      "File \u001b[1;32md:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\pyansys_dqn\\qn_keras.py:38\u001b[0m, in \u001b[0;36mQNetwork.fit_batch\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_batch\u001b[39m(\u001b[39mself\u001b[39m, x, y):\n\u001b[1;32m---> 38\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtrain_on_batch(x, y)\n",
      "File \u001b[1;32md:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\engine\\training.py:2478\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2474\u001b[0m     iterator \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39msingle_batch_iterator(\n\u001b[0;32m   2475\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[0;32m   2476\u001b[0m     )\n\u001b[0;32m   2477\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_train_function()\n\u001b[1;32m-> 2478\u001b[0m     logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   2480\u001b[0m logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2481\u001b[0m \u001b[39mif\u001b[39;00m return_dict:\n",
      "File \u001b[1;32md:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filep31hzrmu.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32md:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\engine\\training.py:1233\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m   1229\u001b[0m     run_step \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfunction(\n\u001b[0;32m   1230\u001b[0m         run_step, jit_compile\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, reduce_retracing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1231\u001b[0m     )\n\u001b[0;32m   1232\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iterator)\n\u001b[1;32m-> 1233\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdistribute_strategy\u001b[39m.\u001b[39;49mrun(run_step, args\u001b[39m=\u001b[39;49m(data,))\n\u001b[0;32m   1234\u001b[0m outputs \u001b[39m=\u001b[39m reduce_per_replica(\n\u001b[0;32m   1235\u001b[0m     outputs,\n\u001b[0;32m   1236\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy,\n\u001b[0;32m   1237\u001b[0m     reduction\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_reduction_method,\n\u001b[0;32m   1238\u001b[0m )\n\u001b[0;32m   1239\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32md:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\engine\\training.py:1222\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1221\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(data):\n\u001b[1;32m-> 1222\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[0;32m   1223\u001b[0m     \u001b[39m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[0;32m   1224\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[1;32md:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\engine\\training.py:1027\u001b[0m, in \u001b[0;36mModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_target_and_loss(y, loss)\n\u001b[0;32m   1026\u001b[0m \u001b[39m# Run backwards pass.\u001b[39;00m\n\u001b[1;32m-> 1027\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mminimize(loss, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainable_variables, tape\u001b[39m=\u001b[39;49mtape)\n\u001b[0;32m   1028\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics(x, y, y_pred, sample_weight)\n",
      "File \u001b[1;32md:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py:527\u001b[0m, in \u001b[0;36m_BaseOptimizer.minimize\u001b[1;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[0;32m    506\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Minimize `loss` by updating `var_list`.\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \n\u001b[0;32m    508\u001b[0m \u001b[39mThis method simply computes gradient using `tf.GradientTape` and calls\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[39m  None\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    526\u001b[0m grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_gradients(loss, var_list, tape)\n\u001b[1;32m--> 527\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_gradients(grads_and_vars)\n",
      "File \u001b[1;32md:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py:1140\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[1;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[0;32m   1138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m skip_gradients_aggregation \u001b[39mand\u001b[39;00m experimental_aggregate_gradients:\n\u001b[0;32m   1139\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maggregate_gradients(grads_and_vars)\n\u001b[1;32m-> 1140\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply_gradients(grads_and_vars, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32md:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py:634\u001b[0m, in \u001b[0;36m_BaseOptimizer.apply_gradients\u001b[1;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[0;32m    633\u001b[0m grads_and_vars \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(grads, trainable_variables))\n\u001b[1;32m--> 634\u001b[0m iteration \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_internal_apply_gradients(grads_and_vars)\n\u001b[0;32m    636\u001b[0m \u001b[39m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \u001b[39mfor\u001b[39;00m variable \u001b[39min\u001b[39;00m trainable_variables:\n",
      "File \u001b[1;32md:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py:1166\u001b[0m, in \u001b[0;36mOptimizer._internal_apply_gradients\u001b[1;34m(self, grads_and_vars)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_internal_apply_gradients\u001b[39m(\u001b[39mself\u001b[39m, grads_and_vars):\n\u001b[1;32m-> 1166\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49m__internal__\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49minterim\u001b[39m.\u001b[39;49mmaybe_merge_call(\n\u001b[0;32m   1167\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_distributed_apply_gradients_fn,\n\u001b[0;32m   1168\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_distribution_strategy,\n\u001b[0;32m   1169\u001b[0m         grads_and_vars,\n\u001b[0;32m   1170\u001b[0m     )\n",
      "File \u001b[1;32md:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py:1216\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn\u001b[1;34m(self, distribution, grads_and_vars, **kwargs)\u001b[0m\n\u001b[0;32m   1213\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_step(grad, var)\n\u001b[0;32m   1215\u001b[0m \u001b[39mfor\u001b[39;00m grad, var \u001b[39min\u001b[39;00m grads_and_vars:\n\u001b[1;32m-> 1216\u001b[0m     distribution\u001b[39m.\u001b[39;49mextended\u001b[39m.\u001b[39;49mupdate(\n\u001b[0;32m   1217\u001b[0m         var, apply_grad_to_update_var, args\u001b[39m=\u001b[39;49m(grad,), group\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m     )\n\u001b[0;32m   1220\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_ema:\n\u001b[0;32m   1221\u001b[0m     _, var_list \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mgrads_and_vars)\n",
      "File \u001b[1;32md:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py:1213\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn.<locals>.apply_grad_to_update_var\u001b[1;34m(var, grad)\u001b[0m\n\u001b[0;32m   1211\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_step_xla(grad, var, \u001b[39mid\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_var_key(var)))\n\u001b[0;32m   1212\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1213\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_step(grad, var)\n",
      "File \u001b[1;32md:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py:224\u001b[0m, in \u001b[0;36m_BaseOptimizer._update_step\u001b[1;34m(self, gradient, variable)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_var_key(variable) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_dict:\n\u001b[0;32m    216\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[0;32m    217\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe optimizer cannot recognize variable \u001b[39m\u001b[39m{\u001b[39;00mvariable\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis usually means you are trying to call the optimizer to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`tf.keras.optimizers.legacy.\u001b[39m\u001b[39m{self.__class__.__name__}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    223\u001b[0m     )\n\u001b[1;32m--> 224\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_step(gradient, variable)\n",
      "File \u001b[1;32md:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\adam.py:200\u001b[0m, in \u001b[0;36mAdam.update_step\u001b[1;34m(self, gradient, variable)\u001b[0m\n\u001b[0;32m    198\u001b[0m     v_hat\u001b[39m.\u001b[39massign(tf\u001b[39m.\u001b[39mmaximum(v_hat, v))\n\u001b[0;32m    199\u001b[0m     v \u001b[39m=\u001b[39m v_hat\n\u001b[1;32m--> 200\u001b[0m variable\u001b[39m.\u001b[39massign_sub((m \u001b[39m*\u001b[39m alpha) \u001b[39m/\u001b[39m (tf\u001b[39m.\u001b[39;49msqrt(v) \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepsilon))\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"d:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"d:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"d:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1027, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"d:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 527, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"d:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 1140, in apply_gradients\n        return super().apply_gradients(grads_and_vars, name=name)\n    File \"d:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 634, in apply_gradients\n        iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"d:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 1166, in _internal_apply_gradients\n        return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"d:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 1216, in _distributed_apply_gradients_fn\n        distribution.extended.update(\n    File \"d:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 1213, in apply_grad_to_update_var  **\n        return self._update_step(grad, var)\n    File \"d:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 224, in _update_step\n        self.update_step(gradient, variable)\n    File \"d:\\repos\\pyansys\\ml-rl-cartpole\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\adam.py\", line 200, in update_step\n        variable.assign_sub((m * alpha) / (tf.sqrt(v) + self.epsilon))\n\n    ValueError: None values not supported.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gym_plotter = LivePlotter()\n",
    "\n",
    "results = dqn_runner.run(env_name,\n",
    "                         dqn.ClassicDQNLearner,\n",
    "                         qn_keras.QNetwork,\n",
    "                         layers=[64, 64],\n",
    "                         n_episodes=200,\n",
    "                         epsilon=dqn_runner.basic_epsilon_linear(1, 0.05, 1000),\n",
    "                         gamma=0.99,\n",
    "                         n_mini_batch=64,\n",
    "                         replay_db_warmup=1000,\n",
    "                         replay_db_capacity=40000,\n",
    "                         c_cycle=1,\n",
    "                         polyak_rate=0.99,\n",
    "                         averaging_window=10,\n",
    "                         victory_threshold=196,\n",
    "                         diagnostics_fn=gym_plotter.live_plot,\n",
    "                         output_path=output_path_gym,\n",
    "                         output_name=output_name_gym)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyAnsys in OpenAI environment\n",
    "\n",
    "Now we do reinforcement learning with MAPDL in the loop as an OpenAI Gym package, thanks to the PyAnsys API.\n",
    "\n",
    "![Fig 6: A single iteration of the CartPole as a Markov Decision Process using MAPDL in pyansys](media/ANSYS_loop.jpg \"Fig 6: A single iteration of the CartPole as a Markov Decision Process using MAPDL in pyansys\")\n",
    "\n",
    "**Fig 6: A single iteration of the CartPole as a Markov Decision Process using MAPDL in pyansys**\n",
    "\n",
    "In this environment, MAPDL will provide the environment response after solving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling PyAnsys Cartpole environment and generating folders for backup.\n",
    "import pyansys_cartpole\n",
    "\n",
    "env_name = 'pyansys-CartPole-v0'\n",
    "tmp_path = tempfile.gettempdir()\n",
    "\n",
    "output_path_pyansys = os.path.join(tmp_path, 'pyansys_cartpole_results', str(uuid4())) \n",
    "output_name_pyansys = 'pyansys_cartpole_00'\n",
    "\n",
    "if not os.path.exists(output_path_pyansys):\n",
    "    os.makedirs(output_path_pyansys)\n",
    "print(f'Writing model in: {output_path_pyansys}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the learner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyansys_plotter = LivePlotter()\n",
    "\n",
    "results2 = dqn_runner.run(env_name,\n",
    "                         dqn.ClassicDQNLearner,\n",
    "                         qn_keras.QNetwork,\n",
    "                         layers=[32, 32],\n",
    "                         n_episodes=5000,\n",
    "                         epsilon=dqn_runner.basic_epsilon_linear(1, 0.05, 1000),\n",
    "                         gamma=0.99,\n",
    "                         n_mini_batch=64,\n",
    "                         replay_db_warmup=1000,\n",
    "                         replay_db_capacity=40000,\n",
    "                         c_cycle=1,\n",
    "                         polyak_rate=0.99,\n",
    "                         averaging_window=10,\n",
    "                         victory_threshold=196,\n",
    "                         diagnostics_fn=pyansys_plotter.live_plot,\n",
    "                         output_path=output_path_pyansys,\n",
    "                         output_name=output_name_pyansys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learner is trained successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Agent\n",
    "For comparison, here we create a simple test agent that behaves randomly and thus is not likely to succeed at the balancing task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required packages\n",
    "from pyansys_dqn.test_agents import RandomAgent, TrainedAgent\n",
    "\n",
    "env_name = 'pyansys-CartPole-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "agent = RandomAgent(env.action_space.n)\n",
    "\n",
    "s, info = env.reset()\n",
    "\n",
    "labels = [\"Cart position\", \"Cart velocity\", \"Theta angle\", \"Pole velocity\"]\n",
    "\n",
    "print(\" - \".join([each.center(20) for each in labels]))\n",
    "print(\" - \".join([ f\"{each:6.3}\".center(20) for each in s]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, notice how we inform the agent about each state transition with `agent.start_state(s)` or `agent.next_reading(s, r, done)` and then ask it to recommend an action with `agent.next_action()`.  We inform the environment this recommendation by feeding the method `env.step(a)`.  We do not expect these recommendations to be good because this agent selects at random from the choices 'left' and 'right', with equal probability.  A control algorithm that just flips a coin to select how to behave is usually not effective.  Thus, the pole should not stay balanced for long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.start_state(s)\n",
    "done, r_tot = False, 0\n",
    "\n",
    "print(\"Action - \" + \" - \".join([each.center(20) for each in labels]))\n",
    "\n",
    "while not done:\n",
    "    a = agent.next_action()\n",
    "    s, r, terminated, truncated, info = env.step(a)\n",
    "    done = truncated or terminated\n",
    "    agent.next_reading(s, r, done, False)\n",
    "    print('--->' if a else '<--- ', ' - ', \" - \".join([ f\"{each:6.3}\".center(20) for each in s]))\n",
    "    r_tot += r\n",
    "\n",
    "print('\\nTotal timesteps:', r_tot+1)\n",
    "print(f'Final theta angle: {s[2]:4.2f} degrees')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see the theta in the last reported state (last row, third column), theta exceed the 12 degrees maximum, hence the simulation is stopped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing trained agent\n",
    "\n",
    "Now we create an agent that has been trained, i.e., that refers to a successful neural networks in order to decide how best to act. It is thus much more likely to perform well and balance the pole for a noticeably greater number of steps... all this despite having a random starting point for the system!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 2\n",
    "agent = TrainedAgent(output_path_pyansys, output_name_pyansys, env.action_space.n, env.observation_space.shape)\n",
    "\n",
    "s, info = env.reset()\n",
    "print(\" - \".join([each.center(20) for each in labels]))\n",
    "print(\" - \".join([ f\"{each:6.3}\".center(20) for each in s]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, notice how we inform the agent about each state transition with `agent.start_state(s)` or `agent.next_reading(s, r, done)` and then ask it to recommend an action with `agent.next_action()`.  We follow its recommendation by feeding it into the environment in `env.step(a)`.  The recommendations should be pretty good because they stem from neural networks that store the information resulting from successful training and the pole should stay up longer, hopefully for the entirety of the episode (200 steps). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.start_state(s)\n",
    "done, r_tot = False, 0\n",
    "\n",
    "# print(\"Action - \" + \" - \".join([each.center(20) for each in labels])) # Uncoment to print each step\n",
    "\n",
    "while not done:\n",
    "    a = agent.next_action()\n",
    "    s, r, terminated, truncated, info = env.step(a)\n",
    "    done = truncated or terminated\n",
    "    agent.next_reading(s, r, done, False)\n",
    "    # print('---> ' if a else '<--- ', ' - ', \" - \".join([ f\"{each:6.3}\".center(20) for each in s])) # Uncomment to print each step\n",
    "    r_tot += r\n",
    "\n",
    "print(\" - \".join([each.center(20) for each in labels]))\n",
    "print(\" - \".join([ f\"{each:6.3}\".center(20) for each in s]))\n",
    "print('\\nTotal timesteps:', r_tot+1)\n",
    "print(f'Final theta angle: {s[2]:4.2f} degrees')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are just printing the last time step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epilogue\n",
    "Try resuming a trained neural network of your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- The DQN Agent - TensofFlow.org - https://www.tensorflow.org/agents/tutorials/0_intro_rl#the_dqn_agent\n",
    "- Human-level control through deep reinforcement learning - V Mnih et al.  - https://www.nature.com/articles/nature14236\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "654adb23138dde8a97090dfd89ed9a2eaa592c610a5d61354a5b563a66dd8e42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
