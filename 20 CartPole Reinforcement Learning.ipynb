{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib\n",
    "matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.12 (default, Oct 12 2021, 03:01:40) [MSC v.1916 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q ./pyansys_rl ./pyansys_gym --user --no-warn-script-location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'logging'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\gayuso\\Other_Projects\\ML_RL_Cartpole\\20 CartPole Reinforcement Learning.ipynb Cell 5'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gayuso/Other_Projects/ML_RL_Cartpole/20%20CartPole%20Reinforcement%20Learning.ipynb#ch0000004?line=5'>6</a>\u001b[0m \u001b[39m# disable tensorflow warnings\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gayuso/Other_Projects/ML_RL_Cartpole/20%20CartPole%20Reinforcement%20Learning.ipynb#ch0000004?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/gayuso/Other_Projects/ML_RL_Cartpole/20%20CartPole%20Reinforcement%20Learning.ipynb#ch0000004?line=7'>8</a>\u001b[0m tf\u001b[39m.\u001b[39;49mlogging\u001b[39m.\u001b[39mset_verbosity(tf\u001b[39m.\u001b[39mlogging\u001b[39m.\u001b[39mERROR)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'logging'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from uuid import uuid4\n",
    "from pyansys_dqn import dqn, dqn_runner, qn_keras\n",
    "\n",
    "# disable tensorflow warnings\n",
    "import tensorflow as tf\n",
    "# tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Reinforcement Learning: DQN</h1>\n",
    "<img src=\"media/DQN_clean.jpg\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "The DQN implementation is inspired by the famous paper [Mnih et al, 2014](https://www.nature.com/articles/nature14236).\n",
    "\n",
    "The algorithm consists of two neural networks (NN) to accumulate the statistical information about optimal reward sequences. \n",
    "One NN, used for learning, is trained at every step by picking a random sample of experiences (state, action, reward)-tuples from an \"experience buffer\".  The buffer is filled from new experiences (old experiences are retired once the contents exceed capacity) as the agent trains.  At the outset, the buffer is pre-filled with experiences from an agent that behaves at random for a fixed number of steps.  The other NN, used for behavior, is just a snapshot of the first refreshed at fixed intervals and dictates what actions to take. \n",
    "\n",
    "Parameters used:\n",
    "* Neural network size/topology:  [4, 32, 32, 2]\n",
    "    * 4: for each components of the state ($x_{\\text{cart}}, v_{\\text{cart}}, \\theta_{\\text{pole}}, v_{\\text{pole}}$)\n",
    "    * 32: hidden layer 1 with 32 ReLU neurons\n",
    "    * 32: hidden layer 2 with 32 ReLU neurons\n",
    "    * 2: for each possible action with linear neurons [Left, Right]\n",
    "* Neural network refresh rate: every timestep\n",
    "* Buffer capacity: 40K experiences\n",
    "* Buffer warmup: 1K experiences\n",
    "* Sample size: 64 experiences per timestep\n",
    "* $\\varepsilon$: linear decay from 1 to .05 in 1000 timestep (.05 thereafter)\n",
    "* $\\gamma = .99$\n",
    "\n",
    "## Learning Task:\n",
    "Try to balance the pole up to 200 steps.  Declare success when the 10-episode average exceeds 196\n",
    "\n",
    "Each learner will display a history of performance per episode, encoded as follows:\n",
    "\n",
    "* e: episode number\n",
    "* n: timesteps \n",
    "* x: max reward thus far\n",
    "* r: reward\n",
    "* v: avg reward (e.g., for last 10 steps)\n",
    "* vx: max average reward\n",
    "* nt: total timesteps thus far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "import collections\n",
    "%matplotlib inline\n",
    "\n",
    "class LivePlotter:\n",
    "    def __init__(self):\n",
    "        self.data = collections.defaultdict(list)\n",
    "        self.figsize=(12,8)\n",
    "        self.title='Training History'  \n",
    "        \n",
    "    def live_plot(self, episode, steps, r_tot, r_max, r_ave, r_ave_max, steps_tot):\n",
    "        self.data['current'].append(r_tot)\n",
    "        self.data['average'].append(r_ave)\n",
    "        \n",
    "        plt.figure(figsize=self.figsize)\n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.data['current'], label='current', color='orange', linestyle='dashed')\n",
    "        plt.plot(self.data['average'], label='average', color='blue', linestyle='-')\n",
    "        if r_ave > 196:\n",
    "            plt.figure(1).text(0.95, 0.05, 'Success!', fontsize=50, color='gray', ha='right', va='bottom', alpha=0.5)\n",
    "            \n",
    "        plt.title(self.title)\n",
    "        plt.grid(True)\n",
    "        plt.xlabel('episode')\n",
    "        plt.ylabel('reward')\n",
    "        plt.legend(loc='center left') # the plot evolves to the right\n",
    "        plt.ylim([0,200])\n",
    "        plt.xlim([0,400])\n",
    "        plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Native\" environment from OpenAI\n",
    "Here we take practice swings running reinforcement learning on a known open source implementation of the classic control problem.\n",
    "You should see the episode rewards and their running average increase until they arrive at the training objective (196 for 10-episode average):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v0'\n",
    "\n",
    "output_path_gym = os.path.join('/tmp/gym_cartpole_results', str(uuid4())) \n",
    "output_name_gym = 'gym_cartpole_00'\n",
    "\n",
    "if not os.path.exists(output_path_gym):\n",
    "    os.makedirs(output_path_gym)\n",
    "print(f'Writing model in: {output_path_gym}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gym_plotter = LivePlotter()\n",
    "# results = dqn_runner.run(env_name,\n",
    "#                          dqn.ClassicDQNLearner,\n",
    "#                          qn_keras.QNetwork,\n",
    "#                          layers=[64, 64],\n",
    "#                          n_episodes=2000,\n",
    "#                          epsilon=dqn_runner.basic_epsilon_linear(1, 0.05, 1000),\n",
    "#                          gamma=0.99,\n",
    "#                          n_mini_batch=64,\n",
    "#                          replay_db_warmup=1000,\n",
    "#                          replay_db_capacity=40000,\n",
    "#                          c_cycle=1,\n",
    "#                          polyak_rate=0.99,\n",
    "#                          averaging_window=10,\n",
    "#                          victory_threshold=196,\n",
    "#                          diagnostics_fn=gym_plotter.live_plot,\n",
    "#                          output_path=output_path_gym,\n",
    "#                          output_name=output_name_gym)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyAnsys environment in OpenAI: MAPDL in the loop\n",
    "Now we do reinforcement learning with MAPDL in the loop as an OpenAI Gym package, thanks to the pyansys API:\n",
    "\n",
    "<img src=\"media/ANSYS_loop.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<center>Fig: A single iteration of the CartPole as a Markov Decision Process using MAPDL in pyansys</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyansys_cartpole\n",
    "\n",
    "env_name = 'pyansys-CartPole-v0'\n",
    "\n",
    "output_path_pyansys = os.path.join('/tmp/pyansys_cartpole_results', str(uuid4())) \n",
    "output_name_pyansys = 'pyansys_cartpole_00'\n",
    "\n",
    "if not os.path.exists(output_path_pyansys):\n",
    "    os.makedirs(output_path_pyansys)\n",
    "print(f'Writing model in: {output_path_pyansys}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyansys_plotter = LivePlotter()\n",
    "\n",
    "results2 = dqn_runner.run(env_name,\n",
    "                         dqn.ClassicDQNLearner,\n",
    "                         qn_keras.QNetwork,\n",
    "                         layers=[32, 32],\n",
    "                         n_episodes=5000,\n",
    "                         epsilon=dqn_runner.basic_epsilon_linear(1, 0.05, 1000),\n",
    "                         gamma=0.99,\n",
    "                         n_mini_batch=64,\n",
    "                         replay_db_warmup=1000,\n",
    "                         replay_db_capacity=40000,\n",
    "                         c_cycle=1,\n",
    "                         polyak_rate=0.99,\n",
    "                         averaging_window=10,\n",
    "                         victory_threshold=196,\n",
    "                         diagnostics_fn=pyansys_plotter.live_plot,\n",
    "                         output_path=output_path_pyansys,\n",
    "                         output_name=output_name_pyansys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
