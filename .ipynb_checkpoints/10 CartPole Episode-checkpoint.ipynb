{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Episodes of CartPole implemented in MAPDL   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./pyansys_rl\n",
      "Processing ./pyansys_gym\n",
      "Collecting gym==0.10.9\n",
      "  Downloading gym-0.10.9.tar.gz (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 8.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras==2.2.4\n",
      "  Downloading Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n",
      "\u001b[K     |████████████████████████████████| 312 kB 84.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16 in /opt/conda/lib/python3.7/site-packages (from pyansys-rl==0.0.1) (1.18.4)\n",
      "Collecting tensorflow==1.13.1\n",
      "  Downloading tensorflow-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (92.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 92.6 MB 12 kB/s s eta 0:00:01     |███████▋                        | 22.2 MB 18.6 MB/s eta 0:00:04\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from gym==0.10.9->pyansys-rl==0.0.1) (1.4.1)\n",
      "Requirement already satisfied: requests>=2.0 in /opt/conda/lib/python3.7/site-packages (from gym==0.10.9->pyansys-rl==0.0.1) (2.23.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gym==0.10.9->pyansys-rl==0.0.1) (1.15.0)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym==0.10.9->pyansys-rl==0.0.1) (1.5.0)\n",
      "Collecting keras-applications>=1.0.6\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 10.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras==2.2.4->pyansys-rl==0.0.1) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from keras==2.2.4->pyansys-rl==0.0.1) (5.3.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from keras==2.2.4->pyansys-rl==0.0.1) (1.1.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1->pyansys-rl==0.0.1) (0.34.2)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1->pyansys-rl==0.0.1) (0.9.0)\n",
      "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
      "  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n",
      "\u001b[K     |████████████████████████████████| 367 kB 83.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astor>=0.6.0\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/jovyan/.local/lib/python3.7/site-packages (from tensorflow==1.13.1->pyansys-rl==0.0.1) (1.33.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1->pyansys-rl==0.0.1) (3.12.1)\n",
      "Collecting tensorboard<1.14.0,>=1.13.0\n",
      "  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 76.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1->pyansys-rl==0.0.1) (0.3.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.13.1->pyansys-rl==0.0.1) (1.1.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0->gym==0.10.9->pyansys-rl==0.0.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0->gym==0.10.9->pyansys-rl==0.0.1) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0->gym==0.10.9->pyansys-rl==0.0.1) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.0->gym==0.10.9->pyansys-rl==0.0.1) (2020.4.5.1)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from pyglet>=1.2.0->gym==0.10.9->pyansys-rl==0.0.1) (0.18.2)\n",
      "Collecting mock>=2.0.0\n",
      "  Downloading mock-4.0.2-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow==1.13.1->pyansys-rl==0.0.1) (46.4.0.post20200518)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1->pyansys-rl==0.0.1) (3.2.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1->pyansys-rl==0.0.1) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1->pyansys-rl==0.0.1) (1.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1->pyansys-rl==0.0.1) (3.1.0)\n",
      "Building wheels for collected packages: pyansys-rl, gym, pyansys-gym\n",
      "  Building wheel for pyansys-rl (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyansys-rl: filename=pyansys_rl-0.0.1-py3-none-any.whl size=7071 sha256=15f001168c3a2221dfbf574aafa4c7d9389e207491cc5a5bb5fcd3cee0aeeba0\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ke24l9g6/wheels/ab/f3/ff/bebff563baa460ac7ab74ef83360389d4ae6f5a05651e162f3\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.10.9-py3-none-any.whl size=1587026 sha256=5fa076ccf3bbebcc581fc5414ae3bd58b76210085e98bee40cf604771b004b3e\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/e7/3d/73/929b7d38e163e1ed40ebb4400fc9a10533221fe694935a52ee\n",
      "  Building wheel for pyansys-gym (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyansys-gym: filename=pyansys_gym-0.0.1-py3-none-any.whl size=10363 sha256=f0bc2857da1ae1d3cb8fe7228dbcde50c14fedd16c00cab99f69dd16b42569eb\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ke24l9g6/wheels/80/64/72/e71acde093f23c47c6e87f82f5841c0c889c0fcfdb32351491\n",
      "Successfully built pyansys-rl gym pyansys-gym\n",
      "Installing collected packages: gym, keras-applications, keras, mock, tensorflow-estimator, astor, tensorboard, tensorflow, pyansys-rl, pyansys-gym\n",
      "  Attempting uninstall: gym\n",
      "    Found existing installation: gym 0.17.2\n",
      "    Uninstalling gym-0.17.2:\n",
      "      Successfully uninstalled gym-0.17.2\n",
      "\u001b[31mERROR: Could not install packages due to an EnvironmentError: [Errno 13] Permission denied: 'RECORD'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ./pyansys_rl ./pyansys_gym -q --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyansys_cartpole'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a224525b9c6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyansys_cartpole\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_printoptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyansys_cartpole'"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import pyansys_cartpole\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Background:  Markov Decision Process</h2>\n",
    "<img src=\"media/MDP_board.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Markov Decision Process we have an agent immersed in an environment.  At any given time, the agent finds itself in a state and it must select one of the available actions.  Upon taking an action, the environment reponds by assigning a reward and transitioning the agent to a successor state.  This loop continues until a terminal state is reached.  It is interesting to ask: could we learn to act optimally in such a setup? could we learn to select sequences of actions that maximize long term cumulative rewards? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"media/MDP_loop.jpg\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>CartPole</h2>\n",
    "<img src=\"media/cartpole_description.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CartPole is a classic control problem.  It is a balancing task: push the cart such that the pinned pole remains upright. In other words, the pole behaves as a solid inverted pendulum and is unstable about the desired configuration.  A simple implementation could use a revolute/hinge joint between the cart and the pole, and a translational joint between the cart and the ground. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>MAPDL in the loop</h3>\n",
    "<img src=\"media/ANSYS_loop.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<center>Fig: A single iteration of the CartPole as a Markov Decision Process using MAPDL</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this implementation of the CartPole as a Markov Decision Process, we highlight the following components:\n",
    "\n",
    "* Actions: push either left (0) or right (1) \n",
    "* State: $x_{\\text{cart}}, v_{\\text{cart}}, \\theta_{\\text{pole}}, v_{\\text{pole}}$\n",
    "* Reward: +1 for every timestep still in equilibrium\n",
    "* Transition Model: courtesy of an MAPDL structural transient analysis \n",
    "\n",
    "At each episode, the system starts in a randomly seeded state, with positions, velocities and angles picked from a uniform distribution about the vertical/resting position, thus it is unlikely to ever be at equilibrium.  Even if it were, the equilibrium would be unstable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance creation: MAPDL in the loop\n",
    "Create an instance of an MAPDL environment that is specially wrapped for use in [OpenAI Gym](https://gym.openai.com/) thanks to the newly developed python gRPC bindings ([pyansys](https://pypi.org/project/pyansys/)).  The wrapper sets up the CartPole physics, accepts the available actions (i.e. forces), and calculates the state transitions (kinematic response) after every time step (an MAPDL load step).  For reference, OpenAI Gym provides its own ad hoc [environment](https://gym.openai.com/envs/CartPole-v1/) for solving the system's [kinematic equations](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'pyansys-CartPole-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run several episodes (e.g., 3) of the CartPole using a random action, i.e., sometimes 0 (push left), sometimes 1 (push right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 3\n",
    "for i in range(n_episodes):\n",
    "    print('*' * 30, f'Episode: {i+1}', '*' * 30)\n",
    "    cur_state = env.reset()\n",
    "    done, r_tot = False, 0\n",
    "    while not done:\n",
    "        action = np.random.choice([0, 1])\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        print('State:', cur_state, '\\tAction:', '--->' if action else '<---', '\\tReward: ', reward)\n",
    "        cur_state, r_tot = next_state, r_tot + reward\n",
    "    print('Episode Reward:', r_tot)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
